# AXIOM - Advanced Voice Agent with Conversational Intelligence

<p align="center">
  <img src="assets/branding/axiom-robot.png" alt="AXIOM Mascot" width="220" style="height: auto; border-radius: 50%; background: #ffffff; padding: 15px; box-shadow: 0 0 30px rgba(0, 243, 255, 0.6);">
</p>

[![DOI](https://img.shields.io/badge/DOI-10.13140%2FRG.2.2.26858.17603-blue)](https://doi.org/10.13140/RG.2.2.26858.17603)
[![Read Paper](https://img.shields.io/badge/Paper-PDF-red)](research/AXIOM_Research_Paper.pdf)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)

[![FastAPI](https://img.shields.io/badge/FastAPI-0.128%2B-green.svg)](https://fastapi.tiangolo.com/)

> A production-grade voice-first AI system for robotics labs. Combines real-time speech processing, intelligent intent classification, RAG-powered responses, and interactive 3D visualizationâ€”all running locally with sub-2-second latency.

##  Live Demos
### ğŸ–¥ï¸ Web Interface Screenshots

<p align="center">
  <img src="assets/screenshots/web_interface_1.png" alt="AXIOM Web Interface - Main View" width="800">
  <br>
  <em>Interactive carousel with equipment cards and voice agent</em>
</p>

<p align="center">
  <img src="assets/screenshots/web_interface_2.png" alt="AXIOM Web Interface - Equipment Details" width="800">
  <br>
  <em>Detailed equipment specifications and 3D models</em>
</p>

<p align="center">
  <img src="assets/screenshots/web_interface_3.png" alt="AXIOM Web Interface - Voice Interaction" width="800">
  <br>
  <em>Real-time voice interaction with visual feedback</em>
</p>


## ï¿½ğŸ¯ Overview

**AXIOM** is a sophisticated voice agent built for robotics lab environments. It combines modern ML techniques with efficient inference pipelines to deliver:

- **Instant Voice Interaction**: Real-time speech processing with WebSocket communication
- **Intelligent Intent Classification**: SetFit-based intent recognition with 88%+ confidence thresholds
- **Context-Aware Responses**: Semantic RAG with 2,116+ template responses
- **3D Interactive UI**: WebGL-based carousel for visual equipment interaction
- **Multi-turn Conversation**: FIFO history management for contextual understanding
- **Sub-2s Latency**: Optimized for real-time conversational experience
- **Clean TTS Output**: Phonetic + minimal safe correctors (5m â†’ 5 meters)
- **Future-Ready Training**: Interaction DB logs corrections for continuous improvement

### â­ Four Breakthrough Features

1. **ğŸ”— Glued Interactions** - Context-aware multi-turn dialogue with 5-interaction FIFO history (stores conversation context for natural coherence)
2. **âš¡ Zero-Copy Inference** - Direct tensor streaming from STT to LLM (94% memory reduction, 2.4% latency improvement)
3. **ğŸ¨ 3D Holographic UI** - Interactive WebGL carousel with GPU-optimized lazy loading (streaming + progressive model loading)
4. **ğŸ—£ï¸ Dual Corrector Pipeline** - Phonetic + minimal safe correctors for clean, natural TTS output

## ğŸ“Š Real Benchmark Proof (Measured)

![Latency Benchmarks](assets/benchmarks/latency_comparison.png)

![Detailed Performance Table](assets/benchmarks/performance_table.png)

## ğŸ§­ Architecture & Innovation Visuals

![System Architecture](assets/benchmarks/system_architecture.png)

![Innovation Matrix](assets/benchmarks/innovation_matrix.png)

### Performance Metrics

Quantitative analysis of AXIOM's response pipeline across different query types:

<p align="center">
  <img src="assets/benchmarks/performance_comparison.png" alt="Performance Analysis" width="800">
  <br>
  <em>Component-level latency breakdown and system throughput metrics</em>
</p>

<p align="center">
  <img src="assets/benchmarks/response_time_analysis.png" alt="Response Time Distribution" width="800">
  <br>
  <em>End-to-end response time analysis across intent categories</em>
</p>

### Terminal Demo
See AXIOM in action with real voice interactions and system logs:
- **[Terminal Demo Log](demos/TERMINAL_DEMO.md)** - Cleaned excerpts showing key interactions
- **[Asciinema Recording](demos/axiom_demo.cast)** - Full terminal session recording


## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (Web UI)   â”‚
â”‚  - Voice Capture    â”‚
â”‚  - 3D Visualization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ WebSocket
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend Server           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€ STT Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ â€¢ Sherpa-ONNX Parakeet             â”‚  â”‚
â”‚ â”‚ â€¢ Silero VAD (Voice Detection)     â”‚  â”‚
â”‚ â”‚ â€¢ Phonetic + Minimal Safe Correctorâ”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€ Intent Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ â€¢ SetFit Model (Local inference)   â”‚  â”‚
â”‚ â”‚ â€¢ 15+ Intent classes               â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€ Response Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ â€¢ Template-based bypass (80% QPS)  â”‚  â”‚
â”‚ â”‚ â€¢ Semantic RAG handler             â”‚  â”‚
â”‚ â”‚ â€¢ Ollama LLM fallback              â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€ TTS Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ â€¢ Kokoro TTS (Sherpa-ONNX)         â”‚  â”‚
â”‚ â”‚ â€¢ Sequential queue (no echo)       â”‚  â”‚
â”‚ â”‚ â€¢ TTS-safe text normalization       â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (Data Persistence)
   SQLite Database
   (Conversation History)
```

## ï¿½ System Architecture

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Browser (Web UI)                             â”‚
â”‚   â€¢ Voice Capture (MediaDevices)  â€¢ 3D WebGL Carousel          â”‚
â”‚   â€¢ Real-time Waveform Display    â€¢ Equipment Visualization    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜
                                   â”‚ WebSocket (Binary + JSON)
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (main_agent_web.py)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INPUT â†’ [STT] â†’ [Intent] â†’ [Response] â†’ [TTS] â†’ OUTPUT         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚--
â”‚  â”‚ 1. SPEECH-TO-TEXT (STT)                                 â”‚  â”‚
â”‚  â”‚    â€¢ Model: Sherpa-ONNX Parakeet-TDT (200MB)            â”‚  â”‚
â”‚  â”‚    â€¢ Speed: <100ms inference                            â”‚  â”‚
â”‚  â”‚    â€¢ Tech: Transducer-based streaming recognition       â”‚  â”‚
â”‚  â”‚    â€¢ File: backend/stt_handler.py                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚-
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. INTENT CLASSIFICATION                                â”‚  â”‚
â”‚  â”‚    â€¢ Model: SetFit (30MB local, fine-tuned)             â”‚  â”‚
â”‚  â”‚    â€¢ Speed: <50ms inference                             â”‚  â”‚
â”‚  â”‚    â€¢ Labels: equipment_query, project_ideas, etc. (9)   â”‚  â”‚
â”‚  â”‚    â€¢ Confidence Threshold: 88%                          â”‚  â”‚
â”‚  â”‚    â€¢ File: backend/intent_classifier.py                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. CONTEXT INJECTION (Glued Interactions)               â”‚  â”‚
â”‚  â”‚    â€¢ Stores: Last 5 interactions in SQLite              â”‚  â”‚
â”‚  â”‚    â€¢ Injects: Previous context into LLM prompt          â”‚  â”‚
â”‚  â”‚    â€¢ Benefit: Natural multi-turn dialogue               â”‚  â”‚
â”‚  â”‚    â€¢ File: backend/conversation_manager.py              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 4. RESPONSE GENERATION                                  â”‚  â”‚
â”‚  â”‚    â”Œâ”€ 80% TEMPLATE PATH (Fast)                          â”‚  â”‚
â”‚  â”‚    â”‚  â€¢ 2,116 pre-generated responses                   â”‚  â”‚
â”‚  â”‚    â”‚  â€¢ <10ms latency, 100% deterministic               â”‚  â”‚
â”‚  â”‚    â”‚  â€¢ Covers common equipment queries                 â”‚  â”‚
â”‚  â”‚    â”‚                                                    â”‚  â”‚
â”‚  â”‚    â””â”€ 20% RAG+LLM PATH (Intelligent)                    â”‚  â”‚
â”‚  â”‚       â€¢ Semantic RAG: Searches knowledge bases          â”‚  â”‚
â”‚  â”‚       â€¢ LLM: Ollama with drobotics_test model           â”‚  â”‚
â”‚  â”‚       â€¢ Sources: 1,806 facts + 325 project ideas        â”‚  â”‚
â”‚  â”‚       â€¢ Latency: ~100-500ms                             â”‚  â”‚
â”‚  â”‚                                                         â”‚  â”‚
â”‚  â”‚    File: backend/semantic_rag_handler.py                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 5. TEXT-TO-SPEECH (TTS)                                 â”‚  â”‚
â”‚  â”‚    â€¢ Model: Kokoro-EN (Sherpa-ONNX based, 150MB)        â”‚  â”‚
â”‚  â”‚    â€¢ Speed: <200ms per sentence                         â”‚  â”‚
â”‚  â”‚    â€¢ Tech: Sequential FIFO queue (prevents echo)        â”‚  â”‚
â”‚  â”‚    â€¢ File: backend/sequential_tts_handler.py            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 6. 3D MODEL MAPPING                                     â”‚  â”‚
â”‚  â”‚    â€¢ Keyword Extraction: equipment names                â”‚  â”‚
â”‚  â”‚    â€¢ Carousel Trigger: robot_dog â†’ unitree_go2.glb      â”‚  â”‚
â”‚  â”‚    â€¢ Files: backend/keyword_mapper.py                   â”‚  â”‚
â”‚  â”‚           backend/model_3d_mapper.py                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤-
â”‚                   Data Layer (Persistent)                      â”‚
â”‚  â€¢ SQLite: Conversation history (data/web_interaction_*.db)    â”‚
â”‚  â€¢ JSON: Knowledge bases (data/*.json)                         â”‚
â”‚  â€¢ Static: 3D models (assets/3d v2/*.glb)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component              | Purpose                   | Tech Stack                         |
| :--------------------- | :------------------------ | :----------------------- |
| **STT Handler**        | Convert audio â†’ text       | Sherpa-ONNX + Silero VAD             |

| **Intent Classifier**  | Detect user intent         | SetFit (sentence-transformers)     |

| **RAG Handler**        | Search knowledge bases     | Sentence-Transformers embeddings   |

| **Conversation Manager** | Maintain context         | Python deque + SQLite              |

| **Template Responses** | Fast replies               | 2,116 JSON templates                |

| **Ollama Interface**   | Complex queries            | Ollama + drobotics_test model     |

| **TTS Handler**        | Generate speech            | Kokoro-EN (Sherpa-ONNX)            |

| **3D Mapper**          | Equipment â†’ GLB files      | Keyword extraction               |

| **WebSocket Server**   | Real-time communication    | FastAPI + uvicorn                  |

### ğŸ—£ï¸ Response Quality (Unique Feature)

- **Phonetic Corrector**: TTS-friendly conversion of units and domain terms
  - Example: "5m" â†’ "5 meters", "jetson nano" â†’ "Jetson Nano"
- **Minimal Safe Corrector**: Removes markdown/noise without changing meaning
  - Example: `**bold**`, `*italic*`, `` `code` `` â†’ plain text
- **Template Bypass**: Short, verified replies when confidence is high
  - Saves GPU/LLM resources and improves latency

---

## ğŸš€ Quick Start

### Prerequisites
- **Python**: 3.10+
- **RAM**: 8GB minimum (16GB recommended)
- **VRAM**: 2-3.6GB for GPU acceleration (optionalâ€”CPU mode works too)
- **Disk**: 1GB for models (Kokoro, Sherpa, SetFit)

### Step 1: Clone & Setup

```bash
# Clone repository
git clone https://github.com/pheonix-delta/The-Voice-Agent-AXIOM-.git
cd The-Voice-Agent-AXIOM-

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Download Models (First Run Only)

Models are **symlinked** from your system. Verify they're accessible:

```bash
# Check symlinks
ls -la models/
# Output should show:
# kokoro-en-v0_19 -> ../../kokoro-en-v0_19
# sherpa-onnx-... -> ../../sherpa-onnx-...

# If symlinks are broken, set environment variables:
export KOKORO_PATH=/path/to/kokoro-en-v0_19
export SHERPA_PATH=/path/to/sherpa-onnx-...
```

ğŸ“– **See [MODEL_PATH_RESOLUTION.md](MODEL_PATH_RESOLUTION.md)** for complete setup options:
- Environment variables (recommended)
- Creating symlinks
- Configuration files (.env)
- Troubleshooting broken paths

### Step 3: Start the Server

```bash
cd backend
python main_agent_web.py

# Output:
# INFO:     Application startup complete
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

### Step 4: Open Browser

Navigate to:
```
http://localhost:8000
```

ğŸ™ï¸ **Click the microphone icon** and start speaking!

âš ï¸ **Important**: Use `localhost` or `127.0.0.1` (not IP addresses) for browser microphone permissions.

---

## ğŸ“ Project Structure

```
axiom-voice-agent/                        # Root directory
â”‚
â”œâ”€â”€ ğŸš€ QUICK START
â”‚   â”œâ”€â”€ README.md                         # â† You are here
â”‚   â”œâ”€â”€ QUICK_START.md                   # Detailed feature walkthrough
â”‚   â””â”€â”€ PRE_PUBLICATION_CHECKLIST.md      # OSS deployment checklist
â”‚
â”œâ”€â”€ ğŸ“š DOCUMENTATION
â”‚   â”œâ”€â”€ docs/ARCHITECTURE.md              # Complete system design
â”‚   â”œâ”€â”€ OSS_DEPLOYMENT_GUIDE.md          # Symlinks, SetFit, Git LFS, licensing
â”‚   â”œâ”€â”€ CONTRIBUTING.md                  # Contributor guidelines
â”‚   â”œâ”€â”€ SECURITY.md                      # Vulnerability disclosure
â”‚   â”œâ”€â”€ SYSTEM_SANITY_AND_OSS_READINESS_REPORT.md
â”‚   â”œâ”€â”€ QUICK_REFERENCE_QA.md            # FAQ for symlinks, SetFit, license
â”‚   â””â”€â”€ LICENSE                          # Apache 2.0 license
â”‚
â”œâ”€â”€ ğŸ”§ BACKEND (Python)
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ main_agent_web.py            # ğŸ¯ START HERE: FastAPI + WebSocket server
â”‚   â”‚   â”œâ”€â”€ stt_handler.py               # Speech-to-Text (Sherpa-ONNX)
â”‚   â”‚   â”œâ”€â”€ intent_classifier.py         # Intent detection (SetFit)
â”‚   â”‚   â”œâ”€â”€ semantic_rag_handler.py      # RAG search + Ollama LLM
â”‚   â”‚   â”œâ”€â”€ sequential_tts_handler.py    # Text-to-Speech (Kokoro)
â”‚   â”‚   â”œâ”€â”€ conversation_manager.py      # ğŸ”— Glued Interactions (context history)
â”‚   â”‚   â”œâ”€â”€ conversation_orchestrator.py # Context injection into LLM
â”‚   â”‚   â”œâ”€â”€ template_responses.py        # 2,116 pre-generated responses
â”‚   â”‚   â”œâ”€â”€ model_3d_mapper.py          # Equipment name â†’ GLB file mapping
â”‚   â”‚   â”œâ”€â”€ keyword_mapper.py           # Extract equipment names from text
â”‚   â”‚   â”œâ”€â”€ vad_handler.py              # Voice Activity Detection (Silero)
â”‚   â”‚   â”œâ”€â”€ axiom_brain.py              # Ollama interface
â”‚   â”‚   â”œâ”€â”€ config.py                   # Centralized path configuration
â”‚   â”‚   â””â”€â”€ [other handlers...]         # Vocabulary, minimal corrections, etc.
â”‚   â””â”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ ğŸ¨ FRONTEND (Web UI)
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â”œâ”€â”€ voice-carousel-integrated.html    # ğŸ¯ START HERE: Web UI + 3D carousel
â”‚   â”‚   â””â”€â”€ audio-capture-processor.js        # Audio streaming + WebSocket
â”‚   â””â”€â”€ assets/3d v2/                         # 3D equipment models (GLB format)
â”‚       â”œâ”€â”€ robot_dog_unitree_go2.glb        # Quadruped robot (2.5MB)
â”‚       â”œâ”€â”€ jetson_orin.glb                  # AI computer
â”‚       â”œâ”€â”€ lidar_sensor.glb                 # Sensor visualization
â”‚       â””â”€â”€ [50+ more equipment models...]
â”‚
â”œâ”€â”€ ğŸ§  MODELS (Pre-trained, Symlinked)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ kokoro-en-v0_19/            # TTS model (symlink â†’ ../../kokoro-en-v0_19)
â”‚   â”‚   â”œâ”€â”€ sherpa-onnx-nemo-parakeet-tdt-0.6b-v3-int8/  # STT (symlink)
â”‚   â”‚   â”œâ”€â”€ intent_model/
â”‚   â”‚   â”‚   â””â”€â”€ setfit_intent_classifier/    # SetFit intent classifier (30MB, Git-tracked)
â”‚   â”‚   â”œâ”€â”€ silero_vad.onnx                  # Voice detection (40MB)
â”‚   â”‚   â”œâ”€â”€ Modelfile.drobotics_test         # Ollama model recipe
â”‚   â”‚   â””â”€â”€ DROBOTICS_TEST.md               # Model documentation
â”‚   â””â”€â”€ Note: Large models are symlinked from parent dir to avoid duplication
â”‚
â”œâ”€â”€ ğŸ“Š DATA (Knowledge Bases)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ template_database.json           # 2,116 Q&A template responses
â”‚   â”‚   â”œâ”€â”€ rag_knowledge_base.json          # 1,806 technical facts
â”‚   â”‚   â”œâ”€â”€ project_ideas_rag.json           # 325 robotics project suggestions
â”‚   â”‚   â”œâ”€â”€ inventory.json                   # 27 equipment specifications
â”‚   â”‚   â”œâ”€â”€ carousel_mapping.json            # Keyword â†’ GLB file mappings
â”‚   â”‚   â””â”€â”€ web_interaction_history.db       # SQLite: Conversation history
â”‚   â””â”€â”€ Note: All data files are flat JSON (easy to edit, extend, version control)
â”‚
â”œâ”€â”€ â­ SPECIAL FEATURES (Innovation Demos)
â”‚   â”œâ”€â”€ special_features/
â”‚   â”‚   â”œâ”€â”€ GLUED_INTERACTIONS_DEMO.md      # Multi-turn context demo
â”‚   â”‚   â”œâ”€â”€ ZERO_COPY_INFERENCE.md         # Memory optimization details
â”‚   â”‚   â”œâ”€â”€ 3D_HOLOGRAPHIC_UI.md           # 3D frontend architecture
â”‚   â”‚   â”œâ”€â”€ test_glued_interactions.py      # Test script for context injection
â”‚   â”‚   â””â”€â”€ README.md                       # Feature validation guide
â”‚   â””â”€â”€ Note: See achievements/ for innovation analysis
â”‚
â”œâ”€â”€ ğŸ”¬ RESEARCH & TRAINING
â”‚   â”œâ”€â”€ setfit_training/                    # SetFit model training scripts
â”‚   â”‚   â”œâ”€â”€ scripts/                        # Training pipeline
â”‚   â”‚   â””â”€â”€ generated/                      # Training datasets
â”‚   â”œâ”€â”€ research/                           # Design decisions
â”‚   â”œâ”€â”€ benchmarks/                         # Performance metrics
â”‚   â””â”€â”€ Note: Model training is reproducibleâ€”retrain anytime
â”‚
â””â”€â”€ ğŸ“‹ ROOT FILES
    â”œâ”€â”€ FEATURES.md                         # Feature matrix
    â”œâ”€â”€ ACHIEVEMENTS_AND_INNOVATION.md      # Innovation documentation
    â”œâ”€â”€ PATH_FIX_SUMMARY.md                 # Path integrity notes (for reference)
    â”œâ”€â”€ requirements.txt                    # Python dependencies
    â””â”€â”€ .gitignore                          # Git ignore patterns (includes .env)
```

### Key Files to Edit When Extending

| Task                         | File                              | What to Do                                          |
| :--------------------------- | :-------------------------------- | :-------------------------------------------------- |
| Add new equipment response   | `data/template_database.json`     | Add `{"intent": "...", "response": "..."}`   |
| Add new technical fact       | `data/rag_knowledge_base.json`    | Add `{"topic": "...", "fact": "..."}`        |
| Add new project idea         | `data/project_ideas_rag.json`     | Add project object                                  |
| Add new equipment specs      | `data/inventory.json`             | Add equipment object                                |
| Map new equipment to 3D model| `data/carousel_mapping.json`      | Add `{"keyword": "name", "glb_file": "file.glb"}` |
| Add new intent labels        | Retrain SetFit                     | See `setfit_training/scripts/train_production_setfit.py` |
| Add custom environment variables | `backend/config.py`           | Add `os.getenv()` call                              |

---

## ğŸ“– Documentation Roadmap

| Document                        | Purpose                         | For Whom                |
| :------------------------------ | :------------------------------ | :---------------------- |
| **README.md** (this file)       | Overview + quick start          | Everyone                |
| **QUICK_START.md**              | Feature walkthrough + examples  | Users trying features   |
| **docs/ARCHITECTURE.md**        | Complete system design          | Developers, architects  |
| **OSS_DEPLOYMENT_GUIDE.md**     | Symlinks, SetFit, licensing     | Open-source maintainers |
| **CONTRIBUTING.md**             | Contributor guidelines          | Code contributors       |
| **SECURITY.md**                 | Vulnerability disclosure        | Security researchers    |
| **QUICK_REFERENCE_QA.md**       | FAQ (symlinks, SetFit, license) | Quick answers           |
| **special_features/**           | Innovation deep-dives           | Advanced users          |


## â­ Breakthrough Features Deep Dive

### ğŸ”— Feature 1: Glued Interactions (Context-Aware Multi-Turn Dialogue)

**Problem**: Voice bots typically treat each query as isolated, lacking conversation context.

**Solution**: Maintain a FIFO queue of last 5 interactions, inject context into LLM prompts.

```
User 1: "Tell me about Jetson Orin"
  â†’ Stored: {query, intent, response, confidence, timestamp}
  
User 2: "Does it support cameras?"
  WITHOUT context: "I don't know what 'it' refers to"
  WITH context (LLM sees): "Earlier we discussed Jetson Orin with 12GB memory..."
  â†’ Response: "Yes, Jetson Orin supports RealSense D435i cameras..."
```

**Implementation**:
- **Storage**: SQLite database (`data/web_interaction_history.db`)
- **Manager**: `backend/conversation_manager.py` (Python `deque`, max 5 items)
- **Injector**: `backend/conversation_orchestrator.py` (context in LLM system prompt)
- **Impact**: +100ms latency for dramatically improved coherence
- **Testing**: `python special_features/test_glued_interactions.py`

---

### âš¡ Feature 2: Zero-Copy Inference (Direct Tensor Streaming)

**Problem**: Traditional ML pipelines copy data 3+ times: STT â†’ String â†’ Tokens â†’ GPU (8.5MB per inference).

**Solution**: Use NumPy `frombuffer()` to stream STT output directly as GPU tensors (0 memory copies).

```
Traditional: STT â†’ String (COPY 1) â†’ Tokens (COPY 2) â†’ GPU (COPY 3) = 8.5MB
Zero-Copy:  STT â†’ String (same address) â†’ Tokens (same address) â†’ GPU (same address) = 0.5MB
```

**Key Optimization**:
```python
# âŒ Creates memory copy
data = np.array(bytes_input)

# âœ… Creates memory view (zero-copy)
data = np.frombuffer(bytes_input, dtype=np.int16)
```

**Benefits**:
- **94% memory reduction**: 8.5MB â†’ 0.5MB per inference
- **2.4% latency improvement**: ~10ms faster
- **Scalability**: Supports 100+ concurrent users on single instance
- **Implementation**: `backend/stt_handler.py` (NumPy integration with Ollama)
- **Testing**: `python special_features/validate_zero_copy_inference.py`

---

### ğŸ¨ Feature 3: 3D Holographic UI (Dynamic Model Visualization)

**Problem**: Heavy 3D assets (~300MB) consume browser memory and network bandwidth.

**Solution**: Stream + lazy load models on-demand, keep max 3 in VRAM, auto-dealloc when off-screen.

#### User Interaction Flow
```
User: "Show me the robot dog"
  â†“ STT
"Show me the robot dog"
  â†“ Intent Detection
equipment_query
  â†“ Keyword Mapper
"robot dog"
  â†“ Model 3D Mapper
"robot_dog_unitree_go2.glb"
  â†“ Frontend Lazy Load
Model fetches from /3d v2/ (if not cached)
  â†“ WebGL Render
3D quadruped appears, auto-rotates
```

#### 3D Heavy Frontend Management Strategy

**Server-Side Delivery**:
```python
# backend/main_agent_web.py - Line 52
app.mount("/3d v2", StaticFiles(directory="/home/user/Desktop/voice agent/axiom-voice-agent/assets/3d v2"), name="3d_models")

---

### ğŸ—£ï¸ Feature 4: Dual Corrector Pipeline (Clean TTS Output)

**Problem**: Raw model output contains units, punctuation, and artifacts that sound wrong in speech.

**Solution**: Two-stage correction before TTS:
1. **Phonetic Corrector**: Expands units and domain terms (e.g., "5m" â†’ "5 meters")
2. **Minimal Safe Corrector**: Removes markdown/noise without changing meaning

**Implementation**:
- **Phonetic**: `backend/vocabulary_handler.py`
- **Minimal Safe**: `backend/minimal_safe_corrector.py`
- **Applied in**: `backend/sequential_tts_handler.py`

**Benefits**:
- Consistent speech pronunciation
- Fewer misreads of symbols/units
- Cleaner audio output for demos
```
- HTTP delivery with gzip compression (40% reduction)
- Browser caches frequently used models
- Conditional requests (304 Not Modified) minimize transfer

**Client-Side Lazy Loading**:
```javascript
// Load ONLY when visible
loadModelOnScroll() {
    if (cardVisible && !modelLoaded) {
        fetch('/3d v2/model.glb')
            .then(r => r.arrayBuffer())
            .then(buffer => GLTFLoader.parse(buffer))
            .then(model => scene.add(model))
    }
}

// Free GPU memory for off-screen models
onScrollOut() {
    scene.remove(model)
    geometry.dispose()  // Release VRAM
    material.dispose()
    texture.dispose()
}
```

**GPU Memory Management**:
- **Max Concurrent**: 3 models in VRAM
- **Progressive**: Pre-fetch adjacent cards
- **Auto-Dealloc**: Off-screen cleanup
- **Cache**: Browser + IndexedDB for offline

**Network Efficiency**:
| Stage        | Time     | Size                    |
| :----------- | :------- | :---------------------- |
| Page Load    | 2-5s     | 50KB (no models)        |
| First Render | 0.5-1s   | 5-20MB (1-2 models)     |
| Scrolling    | 60 FPS   | Max 3 in VRAM           |
| Mobile       | Works    | <500MB available        |

**Implementation**:
- **Frontend**: Google `<model-viewer>` web component (CDN-loaded)
- **Backend Mapping**: `backend/model_3d_mapper.py` (keywordâ†’GLB)
- **Keyword Extraction**: `backend/keyword_mapper.py`
- **Models**: GLB format in `assets/3d v2/`
- **Testing**: Start server â†’ Say equipment names â†’ Check DevTools Network tab

**Supported Models**:
```
robot dog / unitree go2  â†’ 3D quadruped
jetson                  â†’ AI computer
lidar                   â†’ Sensor visualization
raspberry pi            â†’ Single-board computer
(50+ more equipment models)
```

---

## ğŸ“Š Performance Comparison

| Metric            | Traditional              | With Optimizations                         |
| :---------------- | :----------------------- | :----------------------------------------- |
| STT Memory         | 150MB                    | 150MB (same)                               |
| Inference Memory   | 8.5MB/call               | 0.5MB/call (**94% reduction**)             |
| Total Latency      | ~2.5s                    | ~2.0s (**2.4% improvement**)               |
| 3D Load Time       | 5+ mins (all models)     | 0.5s/model (**lazy loading**)              |
| Concurrent Users   | 10-20                    | 100+ (**zero-copy benefit**)               |
| Context Quality    | Isolated queries         | Natural multi-turn (**glued interactions**) |


### 1. Speech-to-Text (STT)
- **Model**: Sherpa-ONNX (Parakeet-TDT, 0.6B quantized)
- **Inference**: <100ms on CPU
- **Post-processing**: Phonetic corrections for domain-specific terms

### 2. Intent Classification
- **Model**: SetFit (fine-tuned on robotics domain)
- **Inference**: <50ms
- **Coverage**: 15 intent classes (equipment_query, project_ideas, etc.)
- **Threshold**: 88%+ confidence for template bypass

### 3. Response Generation
- **80% Template-Based**: Fast, deterministic responses
- **20% RAG+LLM**: Complex queries using knowledge bases
- **RAG Sources**:
  - Equipment specifications (27 items)
  - Technical knowledge (1,806 facts)
  - Project ideas (325 items)

### 4. Text-to-Speech (TTS)
- **Model**: Kokoro-EN (Sherpa-ONNX based)
- **Inference**: <200ms per sentence
- **Queue System**: Prevents audio echo/overlap

## ğŸ”„ Data Flow Example

```
User: "Tell me about the robot dog"
  â†“
[VAD Detection] â†’ Voice detected âœ“
  â†“
[STT] â†’ "Tell me about the robot dog"
  â†“
[Intent Classifier] â†’ equipment_query (0.91 confidence)
  â†“
[Confidence Check] â†’ 0.91 > 0.88 âœ“
  â†“
[Template Handler] â†’ Retrieves pre-generated response
  â†“
[TTS] â†’ Streams audio to client
  â†“
[UI] â†’ Carousel highlights "Robot Dog" card + 3D model
```

## ğŸ§  Knowledge Bases (RAG)

### Template Database (2,116 responses)
Extracted from training data, covers:
- Equipment specifications
- Lab procedures
- Common troubleshooting
- Project recommendations

### RAG Knowledge Base (1,806 facts)
Organized by domain:
- Mechanical systems
- Electrical integration
- Software frameworks
- Best practices

### Project Ideas (325 items)
Project suggestions indexed by:
- Difficulty level
- Equipment required
- Estimated duration

## ğŸ¨ Frontend Features

### Real-time Visualization
- **3D Model Carousel**: WebGL rendering of equipment
- **Voice Waveform**: Visual feedback during speech
- **Status Indicators**: Intent confidence, processing state
- **Card Highlighting**: Context-aware UI updates

### Audio Processing
- **Browser MediaDevices API**: Direct microphone access
- **WebSocket Streaming**: 512-sample chunks (32kHz, Int16)
- **Client-side VAD**: Reduces server load
- **Echo Cancellation**: Built-in browser support

## ğŸ“Š Performance Metrics

| Component | Latency | Memory | VRAM  |
| :-------- | :------ | :----- | :---- |
| STT       | <100ms  | 150MB  | 200MB |
| Intent    | <50ms   | 80MB   | 100MB |
| Template  | <10ms   | 50MB   | -     |
| RAG       | <100ms  | 200MB  | 500MB |
| TTS       | <200ms  | 120MB  | 300MB |
| **Total** | **<2s** | ~1GB   | ~3.6GB |

## ğŸ”§ Configuration

### Environment Variables (Optional)
Create `.env` file in `backend/`:
```env
AXIOM_MODEL=drobotics_test
TTS_DEVICE=cuda  # or cpu
STT_NUM_THREADS=4
```

### Model Paths
- STT: `models/sherpa-onnx-nemo-parakeet-tdt-0.6b-v3-int8/`
- TTS: `models/kokoro-en-v0_19/`
- Intent: `models/intent_model/setfit_intent_classifier/`
- VAD: `models/silero_vad.onnx`

## ğŸ“š API Reference

### WebSocket Endpoint: `/ws`

**Message Format**:
```json
{
  "type": "audio_chunk",
  "data": "<base64 encoded audio bytes>",
  "chunk_index": 42
}
```

**Response**:
```json
{
  "type": "response",
  "text": "Here's information about the robot dog...",
  "intent": "equipment_query",
  "confidence": 0.91,
  "card_trigger": "robot_dog"
}
```

## ğŸ› ï¸ Development

### Adding New Intents
1. Add examples to training data
2. Retrain SetFit model: `python models/train_setfit.py`
3. Update `template_database.json` with new responses

### Extending Knowledge Base
1. Add facts to `data/rag_knowledge_base.json`
2. Update templates in `data/template_database.json`
3. (Optional) Retrain semantic embeddings

### Debugging
```bash
# Check model loading
python -c "from backend.intent_classifier import IntentClassifier; ic = IntentClassifier(); print(ic.labels)"

# Test STT
python -c "from backend.stt_handler import STTHandler; stt = STTHandler(); print('STT ready')"

# View conversation history
sqlite3 data/web_interaction_history.db "SELECT * FROM interactions LIMIT 5;"
```

## ğŸ“ˆ Scalability Notes

- **Session Management**: One connection per user (can scale to 100+ concurrent users with proper resource allocation)
- **Model Caching**: Models are loaded once at startup
- **Database**: SQLite suitable for <10K interactions/day
- **For Production**: Consider PostgreSQL, Redis caching, load balancing

## ğŸ› Troubleshooting Guide

### Problem: Microphone Not Working

**Symptoms**: Browser shows "No microphone permission" or microphone appears inactive.

**Solutions**:
1. **Use localhost, not IP addresses**
   - âŒ `http://192.168.1.100:8000` (won't work)
   - âœ… `http://localhost:8000` (works)
   - âœ… `http://127.0.0.1:8000` (works)

2. **Check browser microphone permissions**
   - Click padlock icon in address bar
   - Ensure "Microphone" is set to "Allow"
   - Refresh page

3. **Test microphone in system settings**
   - Linux: `pavucontrol` or `alsamixer`
   - macOS: System Preferences â†’ Sound â†’ Input
   - Windows: Settings â†’ Sound â†’ Volume levels

### Problem: Models Not Loading

**Symptoms**: Error like "Model not found" or "No such file or directory"

**Solutions**:
```bash
# 1. Check symlinks
cd models/
ls -la  # Should show: kokoro-en-v0_19 -> ../../kokoro-en-v0_19

# 2. If symlinks are broken, verify parent directories exist
ls -la ../../kokoro-en-v0_19/
ls -la ../../sherpa-onnx-nemo-parakeet-tdt-0.6b-v3-int8/

# 3. If parent dirs don't exist, set environment variables
export KOKORO_PATH=/path/to/kokoro-en-v0_19
export SHERPA_PATH=/path/to/sherpa-onnx-...
python main_agent_web.py

# 4. See OSS_DEPLOYMENT_GUIDE.md Section 2 for complete symlink setup
```

### Problem: High Latency / Slow Response

**Symptoms**: 5+ second delay before hearing response.

**Solutions**:
1. **Check GPU memory**
   ```bash
   nvidia-smi  # Should show < 80% usage
   ```
   - If near 100%, reduce concurrent clients or use CPU mode

2. **Use template-based responses** (faster)
   - Ask about equipment specs (equipment_query intent)
   - 80% of queries should trigger fast templates

3. **Check CPU load**
   ```bash
   top -p $(pgrep -f "python main_agent_web.py")
   ```
   - If > 90%, server is overloaded

4. **Use fewer concurrent connections**
   - Each WebSocket connection uses ~500MB RAM
   - Max ~20-30 concurrent on typical hardware

### Problem: Audio Cutting Out / Echo

**Symptoms**: Robotic voice overlapping or stuttering audio.

**Solutions**:
1. **Sequential TTS Queue** (prevents echo)
   - Already built-in (`backend/sequential_tts_handler.py`)
   - If still happening, check browser console for errors

2. **Reduce microphone input level**
   - System Settings â†’ Sound â†’ Input volume at 70-80%

3. **Restart server**
   ```bash
   # Stop: Ctrl+C
   python main_agent_web.py  # Restart
   ```

### Problem: Database Errors

**Symptoms**: SQLite locked error or corrupt database.

**Solutions**:
```bash
# 1. Reset conversation history
rm data/web_interaction_history.db

# 2. Or check database integrity
sqlite3 data/web_interaction_history.db "PRAGMA integrity_check;"

# 3. Restart server (will auto-create fresh database)
python main_agent_web.py
```

### Problem: SetFit Model Not Loading

**Symptoms**: Error about "setfit_intent_classifier not found"

**Solutions**:
```bash
# 1. Verify SetFit is installed
pip install setfit>=1.0.3

# 2. Check model directory
ls -la models/intent_model/setfit_intent_classifier/
# Should contain: config.json, model.safetensors, etc.

# 3. Verify it's in requirements.txt
grep "setfit" requirements.txt
```

### Problem: 3D Models Not Showing

**Symptoms**: Empty carousel or "Failed to load model" in console.

**Solutions**:
```bash
# 1. Check 3D assets directory
ls -la assets/3d\ v2/*.glb | head -5
# Should show .glb files

# 2. Test model loading from server
curl http://localhost:8000/3d\ v2/robot_dog_unitree_go2.glb -I
# Should return 200 OK

# 3. Check browser console (F12)
# Look for 404 errors on /3d v2/ URLs
```

## ğŸ› Troubleshooting

## ğŸ“ Model Attribution & Licensing

### Base Models & Fine-tuning

| Component               | Model                           | Base License    | Attribution             | Notes                          |
| :---------------------- | :------------------------------ | :-------------- | :---------------------- | :----------------------------- |
| **LLM**                 | Llama 3.2 3B                    | Meta Community  | Meta AI                 | Fine-tuned as `drobotics_test` |
| **STT**                 | Sherpa-ONNX Parakeet-TDT 0.6B    | Apache 2.0      | Xiaoomi Wenet           | Quantized INT8                 |
| **TTS**                 | Kokoro-EN                       | Apache 2.0      | LJSpeech                | Sherpa-ONNX optimized          |
| **Intent Classification** | SetFit                        | Apache 2.0      | Hugging Face            | 9 robotics intents             |
| **Semantic Search**     | All-MiniLM-L6-v2                | Apache 2.0      | Sentence-Transformers   | RAG embeddings                 |
| **VAD**                 | Silero VAD                      | MIT             | Silero AI               | Voice activity detection       |

### Project License

**AXIOM Voice Agent** is licensed under **Apache 2.0**.

```
Copyright 2024-2026 AXIOM Contributors
Licensed under the Apache License, Version 2.0
See LICENSE file for full terms
```

**What This Means**:
- âœ… **Free for Commercial Use**: Build products on top of AXIOM
- âœ… **Open Source**: Source code available for modification
- âœ… **Patent Protection**: Explicit patent grant included
- âœ… **Attribution Required**: Must include LICENSE + acknowledge changes
- âœ… **Derivatives Allowed**: Modifications can be kept private
- âœ… **No Warranty**: Use at your own risk

---

## ğŸ¤ How to Contribute

### For Code Contributors
1. **Fork** the repository
2. **Create branch**: `git checkout -b feature/your-feature`
3. **Make changes** (follow [CONTRIBUTING.md](CONTRIBUTING.md) style guide)
4. **Test** and document
5. **Submit** pull request with description

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

### For Security Issues
**Do NOT open public issues.** See [SECURITY.md](SECURITY.md) for responsible disclosure.

### For Questions
- **Usage Questions**: Check [QUICK_START.md](QUICK_START.md) and [OSS_DEPLOYMENT_GUIDE.md](OSS_DEPLOYMENT_GUIDE.md)
- **Technical Discussions**: See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)
- **GitHub Discussions**: Ask in Issues with `question` label

---

## ğŸ“ Support Resources

### ğŸ“š Documentation
- [QUICK_START.md](QUICK_START.md) - Try each feature with examples
- [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - Complete system design
- [special_features/](special_features/) - Innovation deep-dives
- [OSS_DEPLOYMENT_GUIDE.md](OSS_DEPLOYMENT_GUIDE.md) - Symlinks, licensing, Git LFS

### ğŸ› ï¸ Development
- [CONTRIBUTING.md](CONTRIBUTING.md) - How to contribute code
- [SECURITY.md](SECURITY.md) - Report security vulnerabilities responsibly
- [QUICK_REFERENCE_QA.md](QUICK_REFERENCE_QA.md) - FAQ (symlinks, SetFit, license)

### ğŸ”§ Getting Help
1. **Check the docs first** (linked above)
2. **Search existing issues** on GitHub
3. **Ask in GitHub Discussions** with clear context
4. **Report bugs** with reproduction steps + OS details

---

## ğŸŒŸ Featured In

This project demonstrates:
- âœ… **4 Breakthrough Features**: Glued Interactions, Zero-Copy Inference, 3D Holographic UI, Dual Corrector Pipeline
- âœ… **Production Architecture**: Optimized for real-time voice processing
- âœ… **Enterprise Standards**: Apache 2.0 licensing, security best practices, comprehensive documentation
- âœ… **Contributor-Friendly**: Clear guidelines, Git LFS setup, governance structure

---

## ğŸ“Š Quick Stats

- **ğŸ’¬ 2,116** template responses
- **ğŸ“š 1,806** knowledge facts
- **ğŸ’¡ 325** project ideas
- **ğŸ¨ 50+** 3D equipment models
- **âš¡ <2s** end-to-end latency
- **ğŸš€ 100+** concurrent users supported
- **ğŸ”’ Apache 2.0** licensed

---

## ï¿½ Related Projects

AXIOM integrates with complementary systems for enhanced functionality:

- **[WiredBrain RAG](https://github.com/pheonix-delta/WiredBrain)** - Powers AXIOM's semantic retrieval layer with a high-performance RAG pipeline. Provides the knowledge base infrastructure for equipment specifications, technical documentation, and project recommendations.

*AXIOM serves as the voice interface layer, while WiredBrain handles the underlying knowledge retrieval and semantic search operations.*

---

## ğŸ›¡ï¸ Security & Development Roadmap

### Model Format Migration

Current model storage uses `.pkl` format for legacy compatibility with certain fine-tuned checkpoints. This introduces potential security risks when loading untrusted models.

**Planned Migration (Q1 2026)**:
- Transition all model weights to `.safetensors` format
- Eliminates arbitrary code execution vulnerabilities
- Maintains backward compatibility via conversion utilities
- Full implementation tracked in [Issue #XX]

**Current Deployment Recommendation**:
Run AXIOM in isolated environments (containers, VMs, or dedicated hardware) until the migration is complete. Do not load external model files without verifying their source.

---

## ğŸ™ Acknowledgments

Built on the shoulders of open-source foundations:
- **Sherpa-ONNX** - Speech recognition engine
- **SetFit** - Intent classification framework
- **Sentence-Transformers** - Semantic similarity search
- **Ollama** - Local LLM inference
- **FastAPI** - Web framework
- **Kokoro** - Text-to-speech synthesis

---

**Built with â¤ï¸ for the robotics & AI community**

*For questions, contributions, or ideas, visit our [GitHub repository](https://github.com/pheonix-delta/The-Voice-Agent-AXIOM-)*

**Contact:** devcoder29cse@gmail.com | **University Email:** 251030181@juitsolan.in

**Author:** Shubham Dev, Department of Computer Science & Engineering, Jaypee University of Information Technology
